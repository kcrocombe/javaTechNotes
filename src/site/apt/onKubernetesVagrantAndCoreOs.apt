Kubernetes with Vagrant and CoreOS

*Introduction

  The following page describes an effort to get a kubernetes installation up and running from the official coreos vagrant package on GitHub {{https://github.com/coreos/coreos-kubernetes}}. My previous experience with coreos repositories, has been good : basically they seem to work as described. However, this one did seem to be a bit more problematic. The document describes the problems encountered and the extent to which I have solved them.

  Possibly at least some of the problems could stem from the fact that parts of the installation are out of date.  The READMEs provided with the package indicate that the repository I have used is no longer under active development. Instead the user is referred to the {{{https://coreos.com/tectonic/docs/latest/install/aws/index.html} Tectonic Installer}}, which I shall play around with at some point. But for now, I'm going to crack with the older version.


  The guide I was following for this was {{{https://coreos.com/kubernetes/docs/1.6.1/kubernetes-on-vagrant.html}Kubernetes install with Vagrant on CoreOS}}

  Part of the problems, I think, stem from the fact that hte CoreOS product has moved on since the package was released and some of the bits supplied within that distribution are niow supplied in slightly different forms.




*Client kubectl install

  Installed kubectl as follows:

---
  brew install kubectl
---

*Repository Forking.

  As is suggested above, the official package from coroes did not work too well out of th ebox, and I had to make some modifcation to it in order to get anything up and running. Consequently chose to Fork their repository as a basis for my own forward developement.

  Forked via the GitHub GUI

    coreos/coreos-kubernetes --> kcrocombe/coreos-kubernetes

  A local, repository was then forked


---
  ~/dvl/gitClones

  git clone -separate-git-dir=/users/gitRepos/kevin/coreos-kubernetes.git https://github.com/kcrocome/coreos-kubernetes.git
---

 The cloning process creates two notable subdirectories, one each for:

  * single node, and

  * multinode builds:

  []

Attempt1

 Had a go with the single Node and Multi-node options.

 Chose to leave the Container runtime as Docker

 Chose to leave the Network Policy as it was (USE_CALICO=false)


---
  cd <dir>

  vagrant up
---

  Both Failed with:

---
  Can't open /opt/vagrant/embedded/openssl.cnf for reading, No such file or directory

  140735799092096:error:02001002:system library:fopen:No such file or directory:crypto/bio/bss_file.c:74:fopen('/opt/vagrant/embedded/openssl.cnf','r')

  140735799092096:error:2006D080:BIO routines:BIO_new_file:no such file:crypto/bio/bss_file.c:81:
---

  Vagrant ships with an embedded version of openssl. However this version of vagrant, does not seem to include the ssl configuration file that the <<<Vagrantfile>>> is expecting to find.

  Could not find on on the system, so downloaded one from git hub and stuck it where <<<Vagrantfile>>> is expecting to find it.

---
  https://github.com/openssl/openssl/blob/master/apps/openssl.cnf
---

  copied it to: <<</opt/vagrant/embedded/openssl.cnf>>>

  and tried again...


Attempt 2

---
  vagrant up
---

  Now it seems to:

    * generate the ssl artifacts ok

    * configure and initially start a virtual server.

    []

  However, once the newly built server is started and running,  vagrant cannot seem to logon to it in order to complete the configuration.

---
  ==> default: Importing base box 'coreos-alpha'...
  ==> default: Matching MAC address for NAT networking...
  ==> default: Checking if box 'coreos-alpha' version '2331.0.0' is up to date...
  ==> default: Setting the name of the VM: single-node_default_1575408048012_36301
  ==> default: Fixed port collision for 22 => 2222. Now on port 2204.
  ==> default: Clearing any previously set network interfaces...
  ==> default: Preparing network interfaces based on configuration...
    default: Adapter 1: nat
    default: Adapter 2: hostonly
  ==> default: Forwarding ports...
    default: 22 (guest) => 2204 (host) (adapter 1)
  ==> default: Running 'pre-boot' VM customizations...
  ==> default: Booting VM...
  ==> default: Waiting for machine to boot. This may take a few minutes...
    default: SSH address: 127.0.0.1:2204
    default: SSH username: core
    default: SSH auth method: private key
    default: Warning: Connection reset. Retrying...
    default: Warning: Connection reset. Retrying...
    default: Warning: Authentication failure. Retrying...
    default: Warning: Authentication failure. Retrying...
    default: Warning: Authentication failure. Retrying...
---

  attempted to connect manually via ssh:

---
  vagrant ssh core

  vagrant ssh vagrant
---

  Again, failed to connect. So...


Attempt 3

  Within the Vagrantfile, changed Channel from alpha --> stable (within config.rb for the Multinode configuration)

  However this failed with the same result.

Attempt 4

  Within the Vagrantfile, changed Channel from alpha --> beta (within config.rb for the Multinode configuration)

  <<Now, this time it seems to work ok in both Single-Node and Multi-Node options>>

---
  vagrant up

  ==> default: Importing base box 'coreos-beta'...
  ==> default: Matching MAC address for NAT networking...
  ==> default: Checking if box 'coreos-beta' version '2303.2.0' is up to date...
  ==> default: Setting the name of the VM: single-node_default_1575408570878_2596
  ==> default: Fixed port collision for 22 => 2222. Now on port 2204.
  ==> default: Clearing any previously set network interfaces...
  ==> default: Preparing network interfaces based on configuration...
      default: Adapter 1: nat
      default: Adapter 2: hostonly
  ==> default: Forwarding ports...
      default: 22 (guest) => 2204 (host) (adapter 1)
  ==> default: Running 'pre-boot' VM customizations...
  ==> default: Booting VM...
  ==> default: Waiting for machine to boot. This may take a few minutes...
      default: SSH address: 127.0.0.1:2204
      default: SSH username: core
      default: SSH auth method: private key
      default: Warning: Connection reset. Retrying...
      default: Warning: Connection reset. Retrying...
  ==> default: Machine booted and ready!
  ==> default: Configuring and enabling network interfaces...
  ==> default: Running provisioner: file...
      default: /Users/kevin/dvl/gitClones/coreos-kubernetes/single-node/ssl/controller.tar => /tmp/ssl.tar
  ==> default: Running provisioner: shell...
      default: Running: inline script
  ==> default: Running provisioner: file...
      default: /Users/kevin/dvl/gitClones/coreos-kubernetes/single-node/user-data => /tmp/vagrantfile-user-data
  ==> default: Running provisioner: shell...
      default: Running: inline script
---

  Looking at the Vagrantfile and all the files surrounding it, I can find nothing resembling an Ignition Script, or anything which might be injectable into the o/s on its first boot. In particular I can find no reference to any 'core' user or 'vagrant' user which might typically be used to logon to the server.

  Therefore, either the core user already exists in the Beta channel boxfile, (and so doesn't need to be injected in)

  or

  The injection process is injecting ok on the Beta image, but in a way that I don't understand.

  However, I think it is probably the first suggestion: if I change the

      config.ssh.username to something other than "core" and the builds fail!

  So, I think the config is NOT being injected; it is relying on it already being there.

  On other coreos packagings that I have used, the provisioning is performed by a plugin to vagrant. Having checked, this plugin does seem to be available to vagrant:

---
  vagrant plugin list

  vagrant-ignition (0.0.3, global)
---

 However, there does not seem to be anything the Vagrant file that makes use of any of the Ignition features that the plugin provides. All the provisioning seems to be way of inline provisioning scripts, which it can't run because it can't logon to the server.


Fixing the supplied Vagrant File

  In essence what we want to do is to amend the provided Vagrant file so that it uses the vagrant Ignition.plugin. We will then use the vagrant plugin to inject the 'core' userId during system provisioning. In doing this, I basically examined the Vagrant file supplied with the {{{https://github.com/coreos/coreos-vagrant.git}coreOs-vagrant}}, and copied over the relevant ignition sections from there and then configured them as I wanted.

  I think the way that the Ignition plugin works, is basically to set up a drive mapping within the guest os that points to a diskimage  held locally on the host o/s. The diskimage contains the Ignition File that is to be injected into the server as part of provisioning. The location mapped within the guest is one where Ignition defaults to when looking for provisioning scripts. When the Guest O/S gets provisioned, the Ignition System reads the file from the mapped drive and configures the box accordingly.

   If there is no Ignition File, it will create a default one that injects the desired user ("core"), plus public key etc. plus a bit of default network configuration.

   Otherwise, it merges this default info into the custom Ignition file that was provided, so createing a composite one.


 Fixing the supplied Vagrant File


---
  https://github.com/coreos/vagrant-ignition
---



---
  config.ignition.enabled: Set to true to enable this plugin

  config.ignition.path: Set to the path of the base ignition config (can be nil if there is no base)

  config.ignition.config_obj: Set equal to config.vm.provider :virtualbox

  config.ignition.drive_root: Set to desired root directory of generated config drive (optional)

  config.ignition.drive_name: Set to desired filename of generated config drive (optional)

  config.ignition.hostname: Set to desired hostname of the machine (optional)

  config.ignition.ip: Set to desired ip of eth1 (only applies if a private network is being created)
---


     Bit tricky but eventually managed it.



Getting the Single Node Configuration Running.

  Chose to use the Beta Stream initially

*coreos-cloudinit

  The supplied Vagrant file makes use of the {{{https://github.com/coreos/coreos-cloudinit}coreOs-cloudinit}} mechanism for automatically provisioning coreOs with a Kubernetes cluster. Note that cloudinit has now been superceded by Ignition as the preferred provisioning mechanism.

  {{{https://cloudinit.readthedocs.io/en/latest/index.html}CloudInit}} is an indudstry standard didtribution method for machine iniitaliazation. It can initiate provisioning based on instructions provided in various forms, but one of which is a simple script file : user-data

  coreos-cloudinit is cores's implementaion of coloudinit, and it only implements a subset of the full functionality.

  As far as I can tell, teh way that it works is as follows:

    Vagrant injects the user-data script to a particular location on the coreOs intance as it is being provisioned.

---
  /var/lib/coreos-vagrant
---

    When the box starts for the first time, a particular service coreos-cloudinit (or one of a number of related services) runs as a one off. What this does is not at all straightforward to workout, but it is becomes aware of the file in /var/lib/coreos-vagrant and triggers a series of set-up actions including

      creation of the some entries in /etc/environment

      creation of some directories such as /var/lib/

      and eventually moves the user-data file into a different direcory /var/lib/coreos-install/user_data

    Something is watching for this files arrival, and when it does, the

     and calls the /usr/bin/coreos-cloudinit executable get called. The somehow executes the contents of the user_data file and configures the box accordingly.

*Initial startup.

  Amended the Vagrant file to allocate more memory and cpu resources

---
  cpus --> 4
  memory --> 4096
---

  When the box was initially provisioned, the coreos-cloudinit service failed.

---
  Container Linux by CoreOS beta (2303.2.0)

  Failed Units: 1
  coreos-cloudinit-836679022.service
---

  Examination of system journal suggested that the etcd2.service did not exist.

---
  core@localhost ~ $ journalctl  -u coreos-cloudinit-836679022

  journal -xe

  Dec 23 19:51:14 localhost bash[1337]: Failed to enable unit: Unit file etcd2.service does not exist.
  Dec 23 19:51:14 localhost systemd[1]: coreos-cloudinit-836679022.service: Main process exited, code=exited, status=1/FAILURE
  Dec 23 19:51:14 localhost systemd[1]: coreos-cloudinit-836679022.service: Failed with result 'exit-code'.
---


  Some research on the web indicated that the etcd2 service was no longer being provided with this version of coreos and that it had been replaced with v3 etcd, which now expects to be run in a container.

  Amended the user-data file to reflect this change


---
  systemctl enable etcd2; systemctl start etcd2
  -->
  systemctl enable /usr/lib64/systemd/system/etcd-member.service; systemctl start etcd-member
---


  Rebuilt the machine

    vagrant halt

    vagrant destroy

    vagrant up


  This build seemingly succeeded. Slowly, the various container based element of kubernetes came up. It needed several minutes for this.  When complete it seemed to consist of:

    approx 20 docker containers

    approx 4 hyperkube processes

           1 kubelet process

           1 kube-dns process

           1 exechealthz process


  Allowed coreOs to automatically update itself --> beta (2303.2.0)






*The 'Stable' stream

  Then tried the same thing with the 'stable' stream.

  However this did not work at all. It seems that the 'stable' stream is unable to process the injected user-data file in the same way. Nothing in systemd's processing seems to be looking for its arrival...

  (Bit confused by this : if this is an out-dated installation, why does Beta work BUT NOT stable? (I suspect that in this case, the Stable release is actually AHEAD of the Beta Release)

  I'm not going waste anymore time on trying to get this to work... presumably at some point it will work as beta becomes 'stable' ?

*Conformance test

  Checked the installation out by running the conformance-test.sh script. This was not at straightforward as it might be. It does seem to be very resource intensive : had a couple of attempts at it, but it was very slow and a number of tests seemed to be failing with what looked like resource-type issues.

  Attempted to address this by increasing the no of cpu's available to the VM to 4, and the memory available to 4GB.

  Then kicked it off as follows:

    ./conformance-test.sh | tee conformanceTest.log







OK... for now lets just crack on with the remaining steps of the guide paper....

---



   kubectl config set-credentials vagrant-multi-admin \
        --certificate-authority=${PWD}/ssl/ca.pem \
        --client-key=${PWD}/ssl/admin-key.pem \
        --client-certificate=${PWD}/ssl/admin.pem

   User "vagrant-multi-admin" set.


   kubectl config set-context vagrant-multi  \
        --cluster=vagrant-multi-cluster \
        --user=vagrant-multi-admin

   Context "vagrant-multi" created.


   kubectl config use-context vagrant-multi

   Switched to context "vagrant-multi".


   kubectl get nodes
   The connection to the server 172.17.4.101:443 was refused - did you specify the right host or port?
---

  Logged onto the machines in order to try and work out wahta was going on...

    The conttoller server (c1), seems to have started all of its services ok, but seems to be struggling to talk to the etc server e1

---
    journalctl -r | more

  -- Logs begin at Wed 2019-12-04 09:59:38 UTC, end at Wed 2019-12-04 10:06:17 UTC. --
  Dec 04 10:06:17 c1 bash[1395]: Trying: http://172.17.4.51:2379
  Dec 04 10:06:16 c1 bash[1395]: Trying: http://172.17.4.51:2379
  Dec 04 10:06:15 c1 bash[1395]: Trying: http://172.17.4.51:2379
  Dec 04 10:06:14 c1 bash[1395]: Trying: http://172.17.4.51:2379
  Dec 04 10:06:13 c1 bash[1395]: Trying: http://172.17.4.51:2379
  Dec 04 10:06:12 c1 bash[1395]: Trying: http://172.17.4.51:2379
---

  The etc server (e1) (172.17.4.51) failed to start its service:

    user-cloudinit@var-lib-coreos\x2dvagrant-vagrantfile\x2duser\x2ddata.service

  This seems to be because the server does not have the etcd software installed anymore.

 Dec 03 23:19:09 e1 coreos-cloudinit[1279]: 2019/12/03 23:19:09 line 6: warning: deprecated key "etcd2" (etcd2 is no longer shipped in Container Linux)

 Seems to be because etcd is no longer shipped with coreos - should be run in container instead.


 The worker server (w1) also fails to start the one of its services

---
Container Linux by CoreOS beta (2303.2.0)
Failed Units: 2
coreos-cloudinit-255695975.service
update-engine.service
---

  This seems to fail on the

  Dec 04 10:07:53 w1 systemd[1]: coreos-cloudinit-255695975.service: Main process exited, code=exited, status=1/FAILURE

  This looks to be a service that set up the  flanneld.service and kubelet services.

  I suspect the flanned.d service is not starting

---
Dec 04 10:39:27 w1 flannel-wrapper[2029]: timed out
Dec 04 10:39:26 w1 flannel-wrapper[2029]: E1204 10:39:26.778232    2029 main.go:382] Couldn't fetch network config: client: etcd cluster is unavailable or misconfigured; error #0: dial tcp 172.17.4.51:237
9: connect: connection refused
---

  So everything seems to be coming back to the etc.d service not running on the e1 server.






Invstigating trying to get a containerized version of it running.

  This does seem to get things moveing a little. The processes on c1 and w1 seem to managet to talk to it at least...

---
  docker run -d -v /usr/share/ca-certificates/:/etc/ssl/certs -p 4001:4001 -p 2380:2380 -p 2379:2379 \
    --name etcd quay.io/coreos/etcd:v2.3.8 \
    -name etcd0 \
    -advertise-client-urls http://172.17.4.51:2379,http://172.17.4.51:4001 \
    -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 \
    -initial-advertise-peer-urls http://172.17.4.51:2380 \
    -listen-peer-urls http://0.0.0.0:2380 \
    -initial-cluster-token etcd-cluster-1 \
    -initial-cluster etcd0=http://172.17.4.51:2380 \
    -initial-cluster-state new
---

  ..bits of it seem to start working

---
  etcdctl member list

  ea651507af29316f: name=etcd0 peerURLs=http://172.17.4.51:2380 clientURLs=http://172.17.4.51:2379,http://172.17.4.51:4001 isLeader=true
---
    ...but not everthing

---
  kubectl get nodes

  Error from server (NotAcceptable): the server was unable to respond with a content type that the client supports (get nodes)
---

  Some suggestion that this might just be a version compatability thing (which it is, see below)...


  Goint to leave this for now and maybe pick it up later.




Vagrant CoreOS images

    I THINK Vagrant CoreOs box files come with specific additions that are supposed to help provision the machine.

    I THINK these are under:

      /usr/share/oem

    It seems to comprise a .yml file that defines 2 one off Units:
        coreos-cloudinit-vagrant-mkdir.service
        coreos-cloudinit-vagrant-user.path

    It also installs the vagrant insecure key ---> which may explain why the 'beta' stream is loggable onto, while the
    'stable' stream is not. ( See notes and discussion elsewhere)

    It also compriseses a shell script, which seems to be monitoring /etc/envionment for particular changes and fixing it if necessary.

    journalctl --identifier=coreos-cloudinit




    NB ignition is not supported on all platforms



    Converting from Cloudinit to Ignition

    {{{https://github.com/coreos/docs/blob/master/os/migrating-to-clcs.md}Migrating from Cloudinit to Ignition}}




     {{{./onEtcd.html}On the etcd distributed database}}

     curl http://172.17.4.51:2379/v2/machines
http://172.17.4.51:2379




Ways of Testing everything is up and running

  make sure the kubernetes API is available

  On Controller...

---
  curl http://127.0.0.1:8080/version

  {
    "major": "1",
    "minor": "5",
    "gitVersion": "v1.5.4+coreos.0",
    "gitCommit": "97c11b097b1a2b194f1eddca8ce5468fcc83331c",
    "gitTreeState": "clean",
    "buildDate": "2017-03-08T23:54:21Z",
    "goVersion": "go1.7.4",
    "compiler": "gc",
    "platform": "linux/amd64"
  }

  curl -s localhost:10255/pods | jq -r '.items[].metadata.name'

  kube-apiserver-172.17.4.101
  kube-controller-manager-172.17.4.101
  kube-proxy-172.17.4.101
  kube-scheduler-172.17.4.101
---



  On Worker...

---
  curl -s localhost:10255/pods | jq -r '.items[].metadata.name'
kube-proxy-172.17.4.201
---



====

Multi-vagrant

*A Note on Product Versions

  One of the features of CoreOS is that it will normally autmatically update itself to refelct the latest available release on any given stream. Thus it is safe to assume that the OS platform will always be reasonably up to date.

  However that does not necessarily mean all the software run on the platform will remain up to date, because much of these will be downloaded and run in containers.  The exact image (including version) will be specified by whatever mechanism creates the image.

  The fundamental component services of kubernetes are:

    * etcd-member

    * flannel

    * docker

    * kubelet

  Of these, etcd-member and flannel are component services of coreOs itself, and the IMAGE_TAGS specifiying whaich version to use are supplied within the Enironment section of the Service file:

    * Provided in the etcd-member.service file: Environment="ETCD_IMAGE_TAG=v3.3.17"

    * Provided in the flanneld.service file: Environment="FLANNEL_IMAGE_TAG=v0.11A.0"

  Consequently, these SHOULD move forward as the OS moved forward.

  Docker itself, obviously, is not a container. This is also supplied as part of the Container image ( as a gzip file in /usr/share). However it is installed from there to /run/torcx/bin/docker. It is not clear to me whether this gets re-installed when the operating system get installed, though.

      * Current version though is 18.06.3

  The kubernetes main service (kubelet), which exectutes the all-in-one hyperkube executable, is NOT part of the coreOs disributon and is intalled as a drop in service. As before, the particular image that is conainerised when the service is brought up is specified by an envionment variable. This is also presented in the kubelet service file:

      * PRovided in the kubelet.service: Environment=KUBELET_IMAGE_TAG=v1.5.4_coreos.0


  This WILL NOT be updated automatically as the installation moves formward ( since the service file does not form part of the distibution). it will need to be updated manually (presumably by amending the Service file.)

  The current version (1.5.4) is consequently very out of date. ( The latest versions, as of Jan 2020, is 1.17.1; which is some way behind)


    Repository is here:  {{https://quay.io/repository/coreos/hyperkube?tab=tags}}

    which is typically referenced in your container engine. e.g.:
        quay.io/coreos/hyperkube:v1.5.4_coreos.0


    Latest here is v1.9.11_coreos.0  or v1.10.5_coreos.0 ( its a bit hard to tell!)


    Later versions got moved to a different repository at Google:

      {{https://console.cloud.google.com/gcr/images/google-containers/GLOBAL/}}

      Note that specific images for vX.XX.X_coreos no longer seem to be available. Instead all that is supplied is a generic version for a particular architecture(amd64, arm, arm64, ppc64le, s390x. amd64 is equivalen to x86_64, to is the one we would want).

      Note that these are Docker images: not sure if that make any difference when running on rkt?

      So our images would be at

      {{https://console.cloud.google.com/gcr/images/google-containers/GLOBAL/hyperkube-amd64}}

      or possibly

      {{https://console.cloud.google.com/gcr/images/google-containers/GLOBAL/hyperkube}}

    Latest Stable Release:

    curl -sS https://storage.googleapis.com/kubernetes-release/release/stable.txt


      gcr.io/google_containers/hyperkube-{ARCH}:{K8S_VERSION}



*Step by Step Guide for setting up Kubernetes

  Taken from {{{https://github.com/coreos/coreos-kubernetes/blob/master/Documentation/getting-started.md}CoreOs and Kubernetes Step By Step}}

  The primary goals of this guide are:

    * Configure an etcd cluster for Kubernetes to use

    * Generate the required certificates for communication between Kubernetes components

    * Deploy a master node

    * Deploy worker nodes

    * Configure kubectl to work with our cluster

    * Deploy the DNS add-on

    * Deploy the network policy add-on


*Deployment Options

  The following variables will be used throughout this guide. Most of the provided defaults can safely be used, however some values such as ETCD_ENDPOINTS and MASTER_HOST will need to be customised to your infrastructure.

---
  MASTER_HOST=no default
---

  The address of the master node. In most cases this will be the publicly routable IP of the node. Worker nodes must be able to reach the master node(s) via this address on port <<443>>. Additionally, <<external clients>> (such as an administrator using <<<kubectl>>>) will also need access, since this will run the Kubernetes API endpoint.

  If you will be running a high-availability control-plane consisting of multiple master nodes, then MASTER_HOST will ideally be a network load balancer that sits in front of them. Alternatively, a DNS name can be configured which will resolve to the master IPs. How requests are routed to the master nodes will be an important consideration when creating the TLS certificates.

---
  ETCD_ENDPOINTS=no default
---

  List of etcd machines (http://ip:port), comma separated. If you're running a cluster of 5 machines, list them <<all>> here.

  e.g ETCD_ENDPOINTS=http://<Ip1>:2379,http://<Ip2>:2379,http://Ip3>:2379,

  Typically the ETCD_ENDPOINTS will be on http://<someIp>:2379. (The etcd process typically will talk to otehr etcd servers on port: 2380. Donlt get these mixed up).

---
  POD_NETWORK=10.2.0.0/16
---

  The CIDR network to use for pod IPs. Each pod launched in the cluster will be assigned an IP out of this range. This network must be routable between all hosts in the cluster. In a default installation, the <<flannel>> overlay network will provide the routing to this network.

  ({{{./glossary.html#Classless Inter-Domain Routing}Classless Inter-Domain Routing (CIDR)}} wis a method for allocating a range of IP addresses, in this case 10.2.0.0 --> 10.2.254.254)

---
  SERVICE_IP_RANGE=10.3.0.0/24
---

  The {{{./glossary.html#Classless Inter-Domain Routing}CIDR}}  network to use for service cluster VIPs (Virtual IPs). Each service will be assigned a cluster IP out of this range. This must not overlap with <<any>> IP ranges assigned to the POD_NETWORK, or other existing network infrastructure. Routing to these VIPs is handled by a local <<kube-proxy>> service to each host, and are not required to be routable between hosts.

---
  K8S_SERVICE_IP=10.3.0.1
---

  The VIP (Virtual IP) address of the <<Kubernetes API Service>>. If the SERVICE_IP_RANGE is changed above, this must be set to the first IP in that range.

---
  DNS_SERVICE_IP=10.3.0.10
---

  The VIP (Virtual IP) address of the cluster DNS service. This IP <<must>> be in the range of the SERVICE_IP_RANGE and <<cannot>> be the first IP in the range. This <<same>> IP must be configured on all worker nodes to enable DNS service discovery.

*Etcd Cluster Configuration

  There are some {{{./onEtcd.html}general notes on etcd and how it works}} elsewhere, which may be useful in understanding the following.

  This was the trickiest part to configure, mainly because the provided scripts, documentation etc assumes etcd2 server, and the world has moved on to use etcd-member.

  Additional confusion was also caused by the fact that the facilites provided by the 'stable' and 'beta' streams of coreOs are significantly different.

**How the Etcd Provisioning via Vagrant was intended to work.

  By looking at the originally provided Vagrantfile, we can see how the provisioning of the box was intended to work.

    [[1]] Vagrant reads a configuration file etcd-cloud-config.yaml, customizing it very slightly. This principally contains the info needed to configure the etcd2 service.

    [[1]] It then uploads it to /var/lib/coreos-vagrant/vagrant-user-data ( via /tmp)

    [[1]] This is eventually intended to be processed by the coreos-cloudinit service

    [[1]] The coreos-cloudinit calls the coreOs custom binary /usr/bin/coreos-cloudinit, which knows how to process the specific etcd2 stanza in the etcd-cloud-config.yaml and so set up the etcd2 service.

**Why does this not work.

  The basic problems with this are:

    [[1]] This version of coresOs (beta 2303.2.0) does not ship with the etcd2 service. Consequently the coreos-cloudinit service cannot start it.

    [[1]] The version of etcd shipped with beta 2303.2.0 is v3. Additionally, it is expected to run in a container, rather than as a native process. The service, etcd-member,

    [[1]] It is not possible to just change the etcd-cloud-config.yaml file to reference the new service. This is bacause the coreos-cloudinit program which processes the file doesn't know how to set up etcd-member.


**Why Changing to a later version of the operating system does not work.

  It might be expected that changing to a later version of the coreOs operating system would fix these incompatibilities, particularly if we use the "stable" stream.

  The latest release available for vagrant on

      http://stable.release.core-os.net/amd64-usr/

  is 2303.3

  The latest release available for vagrant on

      http://beta.release.core-os.net/amd64-usr/

  is 2303.2 ( i.e. BEHIND the beta stream). There ARE later versions of the beta stream availble (up to 2331.1), but NOT for the vagrant-virtualbox platform.

  Consequently to upgrade at all, we need to switch to the 'Stable' 2303.3 rlease.

  However, this does not work out of the box either, for the following reason

    [[1]] We still have the problem that coreos-cloudinit does not understand how to set up later versions of etcd.

    [[1]] The cloudinit provisioining mechanism no longer seems to work on the 'stable' stream. The full mechanics of the provisioning procews are described here, but the basic change seems to be that the /usr/share/oem files that bootstrap the coreos-cloudinit process are no longer present. This probably reflects the fact that cloudinit is being discontinued as a provisioning process in favour of Ignition.

    [[1]] Because the cloudinit process is not working, we are no longer even getting a core user created. Consequently we can;t even log on.

**Solution

  If we are to move forward with etcd, we have move away from coreos-cloudinit as a provisioning method and look at its replacement : Ignition.

  Looking at the coreos:vagrant repository that is available on github:

  {{https://github.com/coreos/coreos-vagrant}}

  Provisioning of these boxes is via Ignition, and makes use of a plugin to vagrant (vagrant-ignition) to do so. Consequently, is is possible to take the VagrantFile supplied with this repository, and amend it for use for the coreos:kubernetes repository:

  {{https://github.com/coreos/coreos-kerberos}}

  This, therefore, will hopefully provision our etcd server a little differently.

  This repository also seems to provide a configuration file config.ign that looks like it will configure etcd for us via ignition.

  Consequently, we ought to be able to use this as the basis for configuring our own etcd server.

  So:

    [[1]] copy over the config.ign from the coreos-vagrant distribution

    [[1]] Generate a discovery token and amend config.ign to incorporate it. Make sure you specify the right <<size>>

---
  curl https://discovery.etcd.io/new?size=1
---
  Note that the size refers to the number of etcd servers in the cluster, NOT the Number of machines (etcd servers are usually duplicated, in order to provide redundancy). Consequently, in non-production situations, the size is likely to be 1.

  If you re-create the etcd server, the existing token will cease to be valid and you will need to generate a new one.

    [[1]] Amend the Vagrantfile to make use of Ignition.

  This latter step also has the advantage of injecting the core user. So we can log-on once more.


  Ignition seems to have no problem setting up the v3 etcd server.


  The version of etcd that is used is specified within the

      /usr/lib64/systemd/system/etcd-member.service

  via the environment varaible

      ETCD_IMAGE_TAG=v3.3.17


  In other words, the version of etcd is determined by the version of coreOS.


  Once these bits are in place, building a working ETCD server is simply a matter of

  * making sure the Stable stream is used

  * vagrant up


**Running machine

  The running machine will incorporate 2 main services:

    * etcd-member service

    * flanneld service

    []

  Note the flanneld services uses the etcd services to store its network configuration, so this <<WILL NOT>> start until etcd is available.

  Both of these run as images within the rkt container system:

---
  rkt list

  UUID		APP	IMAGE NAME			STATE	CREATED		STARTED		NETWORKS
  4d8863dd	flannel	quay.io/coreos/flannel:v0.11.0	running	7 minutes ago	7 minutes ago
  f9bfdb50	etcd	quay.io/coreos/etcd:v3.3.18	running	7 minutes ago	7 minutes ago
---

  If the etcd server is rebuilt for any reason, then remember that you will need to make sure that the network configuration get reposted to it. It may be that the process building the etcd server will do this for you, but if necessary it can be posted manually. The details may vary, but it will be something like:

---
  curl -X PUT -d "value={\"Network\":\"10.2.0.0/16\",\"Backend\":{\"Type\":\"vxlan\"}}" "172.17.4.51:2379/v2/keys/coreos.com/network/config"
---


***Basic Tests

  Good journals to check:

---
  journalctl -u ignition-setup
  journalctl -u ignition-disks
  journalctl -u ignition-files
  journalctl -u ignition-quench

or

  journalctl -u ignition*


  journalctl -u etcd-member

  journalctl -u flanneld
---

  Quick functionality checks:

---
export ETCDCTL_API=3

etcdctl member list
  2f3900f650797284, started, e1, http://172.17.4.51:2380, http://172.17.4.51:2379

etcdctl put test "Tester"
  OK

etcdctl get test
  test
  Tester
---


*Generation of the SSL Certificates etc for kubernetes controller-node communication.

  The control of entire kubernetes cluster is reliant on various client and server processes making calls to one another. In order to secure the integrity of the cluster, the server processes must be sure that the clients attempting to contact them are legitimate clients.

  The kubernetes client has several ways of validating its clients, but use of certificate-based authentication is widely used.

  There are at least 3 communication paths that need protection:

    * Calls to the API Server  (here protected by apiserver.pem and apiserver-key.pem public/private keys);

    * Calls to the kubelet servers on each of the Worker Nodes (here protected by <\<hostname\>>-worker.pem and <\<hostname\>>-worker-key.pem public/private keys). Note that each worker will have its own certificates.

    * Calls to the Cluster Administration endpoints (here protected by admin.pem and admin-key.pem public/private keys).

    []

  The generation of these keys is summarised below based on the details given {{{https://github.com/coreos/coreos-kubernetes/blob/master/Documentation/openssl.md}here}}.

  There are also scripts available within the coreos/kubernetes git repository that will help in this:

    * {{{https://github.com/coreos/coreos-kubernetes/blob/master/lib/init-ssl}init-ssl}}

    * {{{https://github.com/coreos/coreos-kubernetes/blob/master/lib/init-ssl-ca}}init-ssl-ca}}

    []


**Create a Cluster Root CA

  First, we need to create a new certificate authority which will be used to sign the rest of our certificates.

---
  openssl genrsa -out ca-key.pem 2048

  openssl req -x509 -new -nodes -key ca-key.pem -days 10000 -out ca.pem -subj "/CN=kube-ca"
---

**The Kubernetes API Server Keypair

  This is quite a tricky configuration. You can't actually specify all the options needed via the command line, so a configuration file needs to be used to capture all the requirements.

  This looks like this:

---
[req]
  req_extensions = v3_req
  distinguished_name = req_distinguished_name

  [req_distinguished_name]

  [ v3_req ]
  basicConstraints = CA:FALSE
  keyUsage = nonRepudiation, digitalSignature, keyEncipherment
  subjectAltName = @alt_names

  [alt_names]
  DNS.1 = kubernetes
  DNS.2 = kubernetes.default
  DNS.3 = kubernetes.default.svc
  DNS.4 = kubernetes.default.svc.cluster.local
  IP.1=172.17.4.101
  IP.2=10.3.0.1
---

  Note the two IP Addresses, which are:

    IP.1 = MASTER_HOST
    IP.2 = K8S_SERVICE_IP

  (I don't think there is any relevance to the numbering of the ip addresses : we just need a list of relevant ones)

  If deploying multiple master nodes in an HA configuration, you may need to add more TLS subjectAltNames. Proper configuration of SANs in each certificate depends on how worker nodes and kubectl users contact the master nodes: directly by IP address, via load balancer, or by resolving a DNS name.

---
  Example:

  DNS.5 = ${MASTER_DNS_NAME}
  IP.3 = ${MASTER_IP}
  IP.4 = ${MASTER_LOADBALANCER_IP}
---

  We can then generate the certificates themselves:

---
  openssl genrsa -out apiserver-key.pem 2048

  # First the certificate signing request

  openssl req -new -key apiserver-key.pem -out apiserver.csr -subj "/CN=kube-apiserver" -config openssl.cnf

  #Then the certificate itself(self-signed)

  openssl x509 -req -in apiserver.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out apiserver.pem -days 365 -extensions v3_req -extfile openssl.cnf
---

**Kubernetes Worker Keypairs

  Remember each worker need their own key-pair. However, each can be generated using teh same configuraiton file, but substituting for the environment variable <<<$ENV::WORKER_IP>>>

---
  worker-openssl.cnf

[req]
  req_extensions = v3_req
  distinguished_name = req_distinguished_name

  [req_distinguished_name]

  [ v3_req ]
  basicConstraints = CA:FALSE
  keyUsage = nonRepudiation, digitalSignature, keyEncipherment
  subjectAltName = @alt_names

  [alt_names]
  DNS.1 = kubernetes
  DNS.2 = kubernetes.default
  DNS.3 = kubernetes.default.svc
  DNS.4 = kubernetes.default.svc.cluster.local

  IP.1 = $ENV::WORKER_IP
  #(e.g. 172.17.4.201)
---

  The certificates themselves can then be generated for each Worker Node. We can use the same key for each.

---
  openssl genrsa -out ${WORKER_FQDN}-worker-key.pem 2048
---

  Then for each worker node:

---
  WORKER_IP=${WORKER_IP}

  # First the certificate signing request

  openssl req -new -key ${WORKER_FQDN}-worker-key.pem -out ${WORKER_FQDN}-worker.csr -subj "/CN=${WORKER_FQDN}" -config worker-openssl.cnf

  #Then the certificate itself(self-signed)

  openssl x509 -req -in ${WORKER_FQDN}-worker.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out ${WORKER_FQDN}-worker.pem -days 365 -extensions v3_req -extfile worker-openssl.cnf

---

**The Cluster Administrator Key Pair

  Finally, the pair of keys required for administrative access to the API server (e.g. via kubectl)

---
  openssl genrsa -out admin-key.pem 2048

  # First the certificate signing request
  openssl req -new -key admin-key.pem -out admin.csr -subj "/CN=kube-admin"

  #Then the certificate itself(self-signed)
  openssl x509 -req -in admin.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out admin.pem -days 365

  ( or possibly,

  openssl x509 -req -in admin.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out admin.pem -days 365 -extensions v3_req -extfile admin-req.cnf
  )
---


*Configure the Controller/Master Node.

  Boot a single CoreOS machine which will be used as the Kubernetes master node.  The principle scripts used to build the Controller node should be present in the installation image; you must use a CoreOS version 962.0.0+ for the /usr/lib/coreos/kubelet-wrapper script to be present. See kubelet-wrapper for more information.

  The instructions below configure the required master node components using manifests stored in /etc/kubernetes/manifests. The <<kubelet>> running on the master node will watch this location for new or modified manifests and run them automatically.

  High-availability is achieved by repeating these instructions for each master node. Each of the master components is safe to run on multiple nodes ( although, I believe, only one, (the <<leader>>), is active at any one time.)

  The apiserver itself is stateless, but handles recording the results of leader elections to <<etcd>> on behalf of other master components. The <<controller-manager>> and <<scheduler>> use the leader election mechanism to ensure only one of each is active, leaving the inactive master components ready to assume responsibility in case of failure.


**Install the required certificates

  Create the required directory and place the keys generated previously in the following locations:

---
  sudo mkdir -p /etc/kubernetes/ssl

      File: /etc/kubernetes/ssl/ca.pem
      File: /etc/kubernetes/ssl/apiserver.pem
      File: /etc/kubernetes/ssl/apiserver-key.pem
---

  And make sure you've set proper permission for private key:

---
  sudo chmod 600 /etc/kubernetes/ssl/*-key.pem

  sudo chown root:root /etc/kubernetes/ssl/*-key.pem
---

**Network Configuration

  Networking is provided by Flannel and Calico.

  <<<flannel>>> provides a software-defined overlay network for routing traffic to/from the pods.

  <<<calico>>> secures the overlay network by restricting traffic to/from the pods based on fine-grained network policy.


***Flannel Service

  We will configure flannel to source its local configuration from /etc/flannel/options.env and cluster-level configuration in etcd. Create this file and edit the contents:

    * Replace $\{ADVERTISE_IP\} with this machine's publicly routable IP;

    * Replace $\{ETCD_ENDPOINTS\}

    []

---
  /etc/flannel/options.env

  FLANNELD_IFACE=${ADVERTISE_IP}
  FLANNELD_ETCD_ENDPOINTS=${ETCD_ENDPOINTS}

  e.g.
  FLANNELD_IFACE=172.17.4.101
  FLANNELD_ETCD_ENDPOINTS=http://172.17.4.51:2379
---

  Next create a systemd drop-in, which is a method for appending or overriding parameters of a systemd unit. In this case we're appending two dependency rules. Create the following drop-in, which will use the above configuration when flannel starts:

---
  /etc/systemd/system/flanneld.service.d/40-ExecStartPre-symlink.conf

  [Service]
  ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env
---

***Docker Configuration for Flannel

  In order for flannel to manage the pod network in the cluster, Docker needs to be configured to use it. All we need to do is require that <<flanneld>> is running prior to Docker starting.

  Note: If the pod-network is being managed independently, this step can be skipped. See kubernetes networking for more detail.

  Again, we will use a systemd drop-in:

---
  /etc/systemd/system/docker.service.d/40-flannel.conf

  [Unit]
  Requires=flanneld.service
  After=flanneld.service
  [Service]
  EnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env
---

***Docker Container Network Interface(CNI) options

  Create the Docker CNI Options file referenced in the above:

---
  /etc/kubernetes/cni/docker_opts_cni.env

  DOCKER_OPT_BIP=""
  DOCKER_OPT_IPMASQ=""
---

***Flannel Container Network Interface(CNI) options

  If using Flannel for networking, setup the Flannel CNI configuration with below. If you intend to use Calico for networking, follow the guide to Set Up Calico For Network Policy instead.

---
  /etc/kubernetes/cni/net.d/10-flannel.conf

  {
      "name": "podnet",
      "type": "flannel",
      "delegate": {
          "isDefaultGateway": true
      }
  }
---

**Create the kubelet Unit

  The kubelet is the agent on each machine that starts and stops Pods and other machine-level tasks.

  The kubelet itself usually runs as a container image. The container image in quay.io/coreos/hyperkube, and runs with the name /kubelet within the process tree.

  Because of its central role in orchestrating containers, the kubelet requires special permissions from the host. Because of its better ability to support these special permissions, it is recommended to run the kubelet container using rkt (rather than, say, docker).

  Whenever the control node is started, it is the kubelet that will start the pods making up the server's control plane (apiserver, proxy, controller-manager, scheduler - see below).

  The kubelet communicates with the API server (also running on the master node) with the TLS certificates we placed on disk earlier.

  On the master node, the kubelet is configured to communicate with the API server, but not register for cluster work, as shown in the <<<--register-schedulable=false>>> line in the YAML excerpt below. This prevents user pods being scheduled on the master nodes, and ensures cluster work is routed <<only>> to task-specific worker nodes.

  When using Calico, the kubelet is configured to use the Container Networking Interface (CNI) standard for networking. This makes Calico aware of each pod that is created and allows it to network the pods into the flannel overlay. Both flannel and Calico communicate via CNI interfaces to ensure the correct IP range (managed by flannel) is used for each node.

  Note that the kubelet running on a master node may log repeated attempts to post its status to the API server. These warnings are expected behaviour and can be ignored. Future kubernetes releases plan to handle this common deployment consideration more gracefully.

  Within the following configuration file:

    [[1]] Replace $\{ADVERTISE_IP\} with this node's publicly routable IP;

    [[1]] Replace $\{DNS_SERVICE_IP\};

    [[1]] Replace $\{K8S_VER\} This will map to: quay.io/coreos/hyperkube:$\{K8S_VER\} release, e.g. v1.5.4_coreos.0.

    [[1]] If using Calico for network policy:

        * replace $\{NETWORK_PLUGIN\} with cni

        * Add the following to RKT_RUN_ARGS=

          --volume cni-bin,kind=host,source=/opt/cni/bin \
          --mount volume=cni-bin,target=/opt/cni/bin

        * Add ExecStartPre=/usr/bin/mkdir -p /opt/cni/bin

    [[1]] Decide if you will use additional features such as:

        * mounting ephemeral disks;

        * allow pods to mount RDB or iSCSI volumes;

        * allowing access to insecure container registries;

        * changing your CoreOS auto-update settings;

        []

  Note: Anyone with access to port 10250 on a node can execute arbitrary code in a pod on the node. Information, including logs and metadata, is also disclosed on port 10255. See securing the Kubelet API for more information.

---
  /etc/systemd/system/kubelet.service

  [Service]
  Environment=KUBELET_IMAGE_TAG=${K8S_VER}

  Environment=KUBELET_IMAGE_URL=quay.io/coreos/hyperkube

  Environment="RKT_RUN_ARGS=\
    --uuid-file-save=/var/run/kubelet-pod.uuid \
    --volume var-log,kind=host,source=/var/log \
    --mount volume=var-log,target=/var/log \
    --volume dns,kind=host,source=/etc/resolv.conf \
    --mount volume=dns,target=/etc/resolv.conf"

  ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
  ExecStartPre=/usr/bin/mkdir -p /var/log/containers
  ExecStartPre=/usr/bin/mkdir -p /opt/cni/bin

  ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/run/kubelet-pod.uuid

  ExecStart=/usr/lib/coreos/kubelet-wrapper \
    --api-servers=http://127.0.0.1:8080 \
    --register-schedulable=false \
    --cni-conf-dir=/etc/kubernetes/cni/net.d \
    --network-plugin=${NETWORK_PLUGIN} \
    --container-runtime=docker \
    --allow-privileged=true \
    --pod-manifest-path=/etc/kubernetes/manifests \
    --hostname-override=${ADVERTISE_IP} \
    --cluster_dns=${DNS_SERVICE_IP} \
    --cluster_domain=cluster.local

  ExecStop=-/usr/bin/rkt stop --uuid-file=/var/run/kubelet-pod.uuid

  Restart=always
  RestartSec=10

  [Install]
  WantedBy=multi-user.target

---
  For example:

---
  [Service]
  Environment=KUBELET_IMAGE_TAG=v1.5.4_coreos.0

  Environment=KUBELET_IMAGE_URL=quay.io/coreos/hyperkube

  Environment="RKT_RUN_ARGS=\
      --uuid-file-save=/var/run/kubelet-pod.uuid \
      --volume dns,kind=host,source=/etc/resolv.conf \
      --mount volume=dns,target=/etc/resolv.conf \
      --volume rkt,kind=host,source=/opt/bin/host-rkt \
      --mount volume=rkt,target=/usr/bin/rkt \
      --volume var-lib-rkt,kind=host,source=/var/lib/rkt \
      --mount volume=var-lib-rkt,target=/var/lib/rkt \
      --volume stage,kind=host,source=/tmp \
      --mount volume=stage,target=/tmp \
      --volume var-log,kind=host,source=/var/log \
      --mount volume=var-log,target=/var/log"

  ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
  ExecStartPre=/usr/bin/mkdir -p /opt/cni/bin
  ExecStartPre=/usr/bin/mkdir -p /var/log/containers
  ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/run/kubelet-pod.uuid

  ExecStart=/usr/lib/coreos/kubelet-wrapper   --api-servers=http://127.0.0.1:8080   --register-schedulable=false   --cni-conf-dir=/etc/kubernetes/cni/net.d   --network-plugin=cni   --container-runtime=docker   --rkt-path=/usr/bin/rkt   --rkt-stage1-image=coreos.com/rkt/stage1-coreos   --allow-privileged=true   --pod-manifest-path=/etc/kubernetes/manifests   --hostname-override=172.17.4.101   --cluster_dns=10.3.0.10   --cluster_domain=cluster.local

  ExecStop=-/usr/bin/rkt stop --uuid-file=/var/run/kubelet-pod.uuid
  Restart=always
  RestartSec=10

  [Install]
  WantedBy=multi-user.target
---

**The POD Manifests

  There are a number of kubernetes images that run within containers on the Control Node. These are:

    * kube-apiserver

    * kube-scheduler

    * kube-proxy

    * kube-controller-manager

  Each of these containers host the <<same>> image (hyperkube).  <<Hyperkube is the all-in-one binary for the Kubernetes server components>>. However it is invoked with different parameters, so conferring the behaviours required in each execution context.



***Set Up the kube-apiserver Pod

  The API server is where most of the magic happens. It is stateless by design and takes in API requests, processes them and stores the result in etcd if needed, and then returns the result of the request.

  We're going to use a unique feature of the <<kubelet>> to launch the Pod that runs the API server. Above we configured the kubelet to watch a local directory for pods to run with the --pod-manifest-path=/etc/kubernetes/manifests flag. All we need to do is place our Pod manifest in that location, and the kubelet will make sure it stays running, just as if the Pod was submitted via the API. The cool trick here is that we don't have an API running yet, but the Pod will function the exact same way, which simplifies troubleshooting later on.

  If this is your first time looking at a Pod manifest, don't worry, they aren't all this complicated. But, this shows off the power and flexibility of the Pod concept. Create /etc/kubernetes/manifests/kube-apiserver.yaml with the following settings:

    Replace ${ETCD_ENDPOINTS}
    Replace ${SERVICE_IP_RANGE}
    Replace ${ADVERTISE_IP} with this node's publicly routable IP.

---
  /etc/kubernetes/manifests/kube-apiserver.yaml

  apiVersion: v1
  kind: Pod
  metadata:
    name: kube-apiserver
    namespace: kube-system
  spec:
    hostNetwork: true
    containers:
    - name: kube-apiserver
      image: quay.io/coreos/hyperkube:v1.5.4_coreos.0
      command:
      - /hyperkube
      - apiserver
      - --bind-address=0.0.0.0
      - --etcd-servers=${ETCD_ENDPOINTS}
      - --allow-privileged=true
      - --service-cluster-ip-range=${SERVICE_IP_RANGE}
      - --secure-port=443
      - --advertise-address=${ADVERTISE_IP}
      - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota
      - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
      - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
      - --client-ca-file=/etc/kubernetes/ssl/ca.pem
      - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
      - --runtime-config=extensions/v1beta1/networkpolicies=true
      - --anonymous-auth=false
      livenessProbe:
        httpGet:
          host: 127.0.0.1
          port: 8080
          path: /healthz
        initialDelaySeconds: 15
        timeoutSeconds: 15
      ports:
      - containerPort: 443
        hostPort: 443
        name: https
      - containerPort: 8080
        hostPort: 8080
        name: local
      volumeMounts:
      - mountPath: /etc/kubernetes/ssl
        name: ssl-certs-kubernetes
        readOnly: true
      - mountPath: /etc/ssl/certs
        name: ssl-certs-host
        readOnly: true
    volumes:
    - hostPath:
        path: /etc/kubernetes/ssl
      name: ssl-certs-kubernetes
    - hostPath:
        path: /usr/share/ca-certificates
      name: ssl-certs-host
---

  For example:

---

  kind: Pod
  metadata:
    name: kube-apiserver
    namespace: kube-system
  spec:
    hostNetwork: true
    containers:
    - name: kube-apiserver
      image: quay.io/coreos/hyperkube:v1.5.4_coreos.0
      command:
      - /hyperkube
      - apiserver
      - --bind-address=0.0.0.0
      - --etcd-servers=http://172.17.4.51:2379
      - --allow-privileged=true
      - --service-cluster-ip-range=10.3.0.0/24
      - --secure-port=443
      - --advertise-address=172.17.4.101
      - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota
      - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
      - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
      - --client-ca-file=/etc/kubernetes/ssl/ca.pem
      - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
      - --runtime-config=extensions/v1beta1/networkpolicies=true
      - --anonymous-auth=false
      livenessProbe:
        httpGet:
          host: 127.0.0.1
          port: 8080
          path: /healthz
        initialDelaySeconds: 15
        timeoutSeconds: 15
      ports:
      - containerPort: 443
        hostPort: 443
        name: https
      - containerPort: 8080
        hostPort: 8080
        name: local
      volumeMounts:
      - mountPath: /etc/kubernetes/ssl
        name: ssl-certs-kubernetes
        readOnly: true
      - mountPath: /etc/ssl/certs
        name: ssl-certs-host
        readOnly: true
    volumes:
    - hostPath:
        path: /etc/kubernetes/ssl
      name: ssl-certs-kubernetes
    - hostPath:
        path: /usr/share/ca-certificates
      name: ssl-certs-host
---

***Set Up the kube-proxy Pod

  We're going to run the proxy just like we did the API server. The proxy is responsible for directing traffic destined for specific services and pods to the correct location. The proxy communicates with the API server periodically to keep up to date.

  Both the master and worker nodes in your cluster will run the proxy.

  All you have to do is create /etc/kubernetes/manifests/kube-proxy.yaml, there are no settings that need to be configured.

---
  /etc/kubernetes/manifests/kube-proxy.yaml

  apiVersion: v1
  kind: Pod
  metadata:
    name: kube-proxy
    namespace: kube-system
  spec:
    hostNetwork: true
    containers:
    - name: kube-proxy
      image: quay.io/coreos/hyperkube:v1.5.4_coreos.0
      command:
      - /hyperkube
      - proxy
      - --master=http://127.0.0.1:8080
      - --cluster-cidr=10.2.0.0/16
      securityContext:
        privileged: true
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: ssl-certs-host
        readOnly: true
      - mountPath: /var/run/dbus
        name: dbus
        readOnly: false
    volumes:
    - hostPath:
        path: /usr/share/ca-certificates
      name: ssl-certs-host
    - hostPath:
        path: /var/run/dbus
      name: dbus
---

***Set Up the kube-controller-manager Pod

  The controller manager is responsible for reconciling any required actions based on changes to Replication Controllers.

  For example, if you increased the replica count, the controller manager would generate a scale up event, which would cause a new Pod to get scheduled in the cluster. The controller manager communicates with the API to submit these events.

  Create /etc/kubernetes/manifests/kube-controller-manager.yaml. It will use the TLS certificate placed on disk earlier.

---
  /etc/kubernetes/manifests/kube-controller-manager.yaml

  apiVersion: v1
  kind: Pod
  metadata:
    name: kube-controller-manager
    namespace: kube-system
  spec:
    hostNetwork: true
    containers:
    - name: kube-controller-manager
      image: quay.io/coreos/hyperkube:v1.5.4_coreos.0
      command:
      - /hyperkube
      - controller-manager
      - --master=http://127.0.0.1:8080
      - --leader-elect=true
      - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
      - --root-ca-file=/etc/kubernetes/ssl/ca.pem
      resources:
        requests:
          cpu: 200m
      livenessProbe:
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10252
        initialDelaySeconds: 15
        timeoutSeconds: 15
      volumeMounts:
      - mountPath: /etc/kubernetes/ssl
        name: ssl-certs-kubernetes
        readOnly: true
      - mountPath: /etc/ssl/certs
        name: ssl-certs-host
        readOnly: true
    volumes:
    - hostPath:
        path: /etc/kubernetes/ssl
      name: ssl-certs-kubernetes
    - hostPath:
        path: /usr/share/ca-certificates
      name: ssl-certs-host
---

***Set Up the kube-scheduler Pod

  The scheduler monitors the API for unscheduled pods, finds them a machine to run on, and then communicates the decision back to the API.

  Create File /etc/kubernetes/manifests/kube-scheduler.yaml:

---
  /etc/kubernetes/manifests/kube-scheduler.yaml

  apiVersion: v1
  kind: Pod
  metadata:
    name: kube-scheduler
    namespace: kube-system
  spec:
    hostNetwork: true
    containers:
    - name: kube-scheduler
      image: quay.io/coreos/hyperkube:v1.5.4_coreos.0
      command:
      - /hyperkube
      - scheduler
      - --master=http://127.0.0.1:8080
      - --leader-elect=true
      resources:
        requests:
          cpu: 100m
      livenessProbe:
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10251
        initialDelaySeconds: 15
        timeoutSeconds: 15
---

**Set Up Calico For Network Policy (optional)

  This step can be skipped if you do not wish to provide network policy to your cluster using Calico.

  Details can be found {{{https://github.com/coreos/coreos-kubernetes/blob/master/Documentation/deploy-master.md}here}} if needed.


*Start Controller Node Services

  Now that we've defined all of our units and written our TLS certificates to disk, we're ready to start the master components.

  Load Changed Units

  First, we need to tell systemd that we've changed units on disk and it needs to rescan everything:

---
  sudo systemctl daemon-reload
---

**Write the flannel network configuration to etcd.

  Earlier it was mentioned that flannel stores cluster-level configuration in etcd. We need to configure our Pod network IP range within it now:

    * Replace $POD_NETWORK;

    * Replace $ETCD_SERVER with one url (http://ip:port) from $ETCD_ENDPOINTS;

    []

---
  curl -X PUT -d "value={\"Network\":\"$POD_NETWORK\",\"Backend\":{\"Type\":\"vxlan\"}}" "$ETCD_SERVER/v2/keys/coreos.com/network/config"
---

**Restart the flannel service

  After configuring flannel, we should restart it for our changes to take effect. Note that this will also restart the docker daemon and could impact running containers.

---
  sudo systemctl start flanneld

  sudo systemctl enable flanneld
---

**Start the kubelet service

  Now that everything is configured, we can start the kubelet, which will also start the Pod manifests for the API server, the controller manager, proxy and scheduler.

---
  sudo systemctl start kubelet
---

  Ensure that the kubelet will start after a reboot:

---
  sudo systemctl enable kubelet
---

*Basic Controller Health Checks

  Theoretically, our control node is now fully built and active. we can make some basic checks to it as follows:

  First, we need to make sure the Kubernetes API is available (this could take a few minutes after starting the kubelet.service)

---
  curl http://127.0.0.1:8080/version
---

  A successful response should look something like:

---
  {
    "major": "1",
    "minor": "4",
    "gitVersion": "v1.5.2+coreos.0",
    "gitCommit": "ec2b52fabadf824a42b66b6729fe4cff2c62af8c",
    "gitTreeState": "clean",
    "buildDate": "2016-11-14T19:42:00Z",
    "goVersion": "go1.6.3",
    "compiler": "gc",
    "platform": "linux/amd64"
  }
---

  To check the health of the kubelet systemd unit that we created,

---
  systemctl status kubelet.service.
---

  Our Pods should now be starting up, downloading their containers. Once the kubelet has started, you can check that  it has created its pods via the metadata api:

---
  curl -s localhost:10255/pods | jq -r '.items[].metadata.name'

  kube-scheduler-$node
  kube-apiserver-$node
  kube-controller-$node
  kube-proxy-$node
---

  curl -s localhost:10255/pods | jq

======

*Configure the Worker Nodes

  Boot one or more CoreOS nodes which will be used as Kubernetes Workers. The principle scripts used to set up the Worker node shouls already be present in the installation; though you must use a CoreOS version 962.0.0+ for the /usr/lib/coreos/kubelet-wrapper script to be present. See kubelet-wrapper for more information.


**Install the required certificates.

  Place the TLS keypairs generated above in the following locations. Note that each keypair is unique to the worker and should be installed on the worker node it was specifically generated for:

---
    File: /etc/kubernetes/ssl/ca.pem
    File: /etc/kubernetes/ssl/${WORKER_FQDN}-worker.pem
    File: /etc/kubernetes/ssl/${WORKER_FQDN}-worker-key.pem
---

    And make sure you've set proper permission for private key:

---
  sudo chmod 600 /etc/kubernetes/ssl/*-key.pem
  sudo chown root:root /etc/kubernetes/ssl/*-key.pem
---

  Create symlinks to the worker-specific certificate and key so that the remaining configurations on the workers do not have to be unique per worker.

---
  cd /etc/kubernetes/ssl/

  sudo ln -s ${WORKER_FQDN}-worker.pem worker.pem

  sudo ln -s ${WORKER_FQDN}-worker-key.pem worker-key.pem
---

**Networking Configuration

***Flannel Service

  Just like earlier, create the /etc/flannel/options.env and modify these values:

    * Replace ${ADVERTISE_IP} with this node's publicly routable IP.

    * Replace ${ETCD_ENDPOINTS}

    []

---
  /etc/flannel/options.env

  FLANNELD_IFACE=${ADVERTISE_IP}
  FLANNELD_ETCD_ENDPOINTS=${ETCD_ENDPOINTS}
---

  e.g.

---
  FLANNELD_IFACE=172.17.4.201
  FLANNELD_ETCD_ENDPOINTS=http://172.17.4.51:2379
---

  Next create a systemd drop-in, which will use the above configuration when flannel starts

---
  /etc/systemd/system/flanneld.service.d/40-ExecStartPre-symlink.conf

  [Service]
  ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env
---

***Docker Configuration for Flannel

  We need to 'require' that flanneld is running prior to Docker start.

  Create /etc/systemd/system/docker.service.d/40-flannel.conf

---
  /etc/systemd/system/docker.service.d/40-flannel.conf

  [Unit]
  Requires=flanneld.service
  After=flanneld.service
  [Service]
  EnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env
---

***Docker Container Network Interface(CNI) options

  Create the Docker CNI Options file referenced in the above:

---
  /etc/kubernetes/cni/docker_opts_cni.env

  DOCKER_OPT_BIP=""
  DOCKER_OPT_IPMASQ=""
---

  Setup the Flannel CNI configuration with below. If you intend to use Calico for networking, setup using Set Up the CNI config (optional) instead.

---
  /etc/kubernetes/cni/net.d/10-flannel.conf

  {
      "name": "podnet",
      "type": "flannel",
      "delegate": {
          "isDefaultGateway": true
      }
  }
---

**Create the kubelet Unit

  The kubelet is the agent on each machine that starts and stops Pods and other machine-level tasks.

  The kubelet itself usually runs as a container image. The container image in quay.io/coreos/hyperkube, and runs with the name /kubelet within the process tree.

  Because of its central role in orchestrating containers, the kubelet requires special permissions from the host. Because of its better ability to support these special permissions, it is recommended to run the kubelet container using rkt (rather than, say, docker).

  When the worker node is started, it is the kubelet that will start the control plane components on the worker.

  Create /etc/systemd/system/kubelet.service and substitute the following variables:

      * Replace $/{MASTER_HOST/};

      * Replace $/{ADVERTISE_IP/} with this node's publicly routable IP;

      * Replace $/{DNS_SERVICE_IP/};

      * Replace $/{K8S_VER/} This will map to: quay.io/coreos/hyperkube:$/{K8S_VER/} release, e.g. v1.5.4_coreos.0;

      * If using Calico for network policy:

        * Replace $/{NETWORK_PLUGIN/} with cni;

        * Add the following to RKT_RUN_ARGS=\
              --volume cni-bin,kind=host,source=/opt/cni/bin \
              --mount volume=cni-bin,target=/opt/cni/bin

        * Add ExecStartPre=/usr/bin/mkdir -p /opt/cni/bin

        []

      * Decide if you will use additional features such as:

        * mounting ephemeral disks;

        * allow pods to mount RDB or iSCSI volumes;

        * allowing access to insecure container registries;

        * changing your CoreOS auto-update settings;

        []
      []

  Note: Anyone with access to port 10250 on a node can execute arbitrary code in a pod on the node. Information, including logs and metadata, is also disclosed on port 10255. See securing the Kubelet API for more information.

---
  /etc/systemd/system/kubelet.service

  [Service]
  Environment=KUBELET_IMAGE_TAG=${K8S_VER}
  Environment=KUBELET_IMAGE_URL=quay.io/coreos/hyperkube

  Environment=\
    "RKT_RUN_ARGS=--uuid-file-save=/var/run/kubelet-pod.uuid   \
      --volume dns,kind=host,source=/etc/resolv.conf   \
      --mount volume=dns,target=/etc/resolv.conf   \
      --volume rkt,kind=host,source=/opt/bin/host-rkt   \
      --mount volume=rkt,target=/usr/bin/rkt   \
      --volume var-lib-rkt,kind=host,source=/var/lib/rkt   \
      --mount volume=var-lib-rkt,target=/var/lib/rkt   \
      --volume stage,kind=host,source=/tmp   \
      --mount volume=stage,target=/tmp   \
      --volume var-log,kind=host,source=/var/log   \
      --mount volume=var-log,target=/var/log"

  ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
  ExecStartPre=/usr/bin/mkdir -p /var/log/containers
  ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/run/kubelet-pod.uuid

  ExecStart=/usr/lib/coreos/kubelet-wrapper \
    --api-servers=https://${MASTER_HOST} \
    --cni-conf-dir=/etc/kubernetes/cni/net.d \
    --network-plugin=${NETWORK_PLUGIN} \
    --container-runtime=docker \
    --rkt-path=/usr/bin/rkt   \
    --rkt-stage1-image=coreos.com/rkt/stage1-coreos   \
    --register-node=true \
    --allow-privileged=true \
    --pod-manifest-path=/etc/kubernetes/manifests \
    --hostname-override=${ADVERTISE_IP} \
    --cluster_dns=${DNS_SERVICE_IP} \
    --cluster_domain=cluster.local \
    --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml \
    --tls-cert-file=/etc/kubernetes/ssl/worker.pem \
    --tls-private-key-file=/etc/kubernetes/ssl/worker-key.pem

  ExecStop=-/usr/bin/rkt stop --uuid-file=/var/run/kubelet-pod.uuid
  Restart=always
  RestartSec=10

  [Install]
  WantedBy=multi-user.target
---

  For example,

---
  [Service]
  Environment=KUBELET_IMAGE_TAG=v1.5.4_coreos.0
  Environment=KUBELET_IMAGE_URL=quay.io/coreos/hyperkube
  Environment="RKT_RUN_ARGS=--uuid-file-save=/var/run/kubelet-pod.uuid   --volume dns,kind=host,source=/etc/resolv.conf   --mount volume=dns,target=/etc/resolv.conf   --volume rkt,kind=host,source=/opt/bin/host-rkt   --mount volume=rkt,target=/usr/bin/rkt   --volume var-lib-rkt,kind=host,source=/var/lib/rkt   --mount volume=var-lib-rkt,target=/var/lib/rkt   --volume stage,kind=host,source=/tmp   --mount volume=stage,target=/tmp   --volume var-log,kind=host,source=/var/log   --mount volume=var-log,target=/var/log   "
  ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
  ExecStartPre=/usr/bin/mkdir -p /var/log/containers
  ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/run/kubelet-pod.uuid
  ExecStartPre=/usr/bin/mkdir -p /opt/cni/bin
  ExecStart=/usr/lib/coreos/kubelet-wrapper   --api-servers=http://172.17.4.101:443   --cni-conf-dir=/etc/kubernetes/cni/net.d   --network-plugin=cni   --container-runtime=docker   --rkt-path=/usr/bin/rkt   --rkt-stage1-image=coreos.com/rkt/stage1-coreos   --register-node=true   --allow-privileged=true   --pod-manifest-path=/etc/kubernetes/manifests   --hostname-override=172.17.4.201   --cluster_dns=10.3.0.10   --cluster_domain=cluster.local   --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml   --tls-cert-file=/etc/kubernetes/ssl/worker.pem   --tls-private-key-file=/etc/kubernetes/ssl/worker-key.pem
  ExecStop=-/usr/bin/rkt stop --uuid-file=/var/run/kubelet-pod.uuid
  Restart=always
  RestartSec=10

  [Install]
  WantedBy=multi-user.target
---

**The POD Manifest

  The worker node runs just one controll plane pod:

    * kube-proxy

  As with the Controller node, this runs the hyperkube image with suitable parameters to modify its behaviour.


***Set Up the kube-proxy Pod

  Create /etc/kubernetes/manifests/kube-proxy.yaml:

    Replace $\{MASTER_HOST\}

---
  /etc/kubernetes/manifests/kube-proxy.yaml

  apiVersion: v1
  kind: Pod
  metadata:
    name: kube-proxy
    namespace: kube-system
    annotations:
      rkt.alpha.kubernetes.io/stage1-name-override: coreos.com/rkt/stage1-fly
  spec:
    hostNetwork: true
    containers:
    - name: kube-proxy
      image: quay.io/coreos/hyperkube:v1.5.4_coreos.0
      command:
      - /hyperkube
      - proxy
      - --master=${MASTER_HOST}
      - --cluster-cidr=10.2.0.0/16
      - --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml
      securityContext:
        privileged: true
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: "ssl-certs"
      - mountPath: /etc/kubernetes/worker-kubeconfig.yaml
        name: "kubeconfig"
        readOnly: true
      - mountPath: /etc/kubernetes/ssl
        name: "etc-kube-ssl"
        readOnly: true
      - mountPath: /var/run/dbus
        name: dbus
        readOnly: false
    volumes:
    - name: "ssl-certs"
      hostPath:
        path: "/usr/share/ca-certificates"
    - name: "kubeconfig"
      hostPath:
        path: "/etc/kubernetes/worker-kubeconfig.yaml"
    - name: "etc-kube-ssl"
      hostPath:
        path: "/etc/kubernetes/ssl"
    - hostPath:
        path: /var/run/dbus
      name: dbus
---

  For example:

---
apiVersion: v1
kind: Pod
metadata:
name: kube-proxy
namespace: kube-system
annotations:
  rkt.alpha.kubernetes.io/stage1-name-override: coreos.com/rkt/stage1-fly
spec:
hostNetwork: true
containers:
- name: kube-proxy
  image: quay.io/coreos/hyperkube:v1.5.4_coreos.0
  command:
  - /hyperkube
  - proxy
  - --master=http://172.17.4.101:443
  - --cluster-cidr=10.2.0.0/16
  - --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml
  securityContext:
    privileged: true
  volumeMounts:
  - mountPath: /etc/ssl/certs
    name: "ssl-certs"
  - mountPath: /etc/kubernetes/worker-kubeconfig.yaml
    name: "kubeconfig"
    readOnly: true
  - mountPath: /etc/kubernetes/ssl
    name: "etc-kube-ssl"
    readOnly: true
  - mountPath: /var/run/dbus
    name: dbus
    readOnly: false
volumes:
- name: "ssl-certs"
  hostPath:
    path: "/usr/share/ca-certificates"
- name: "kubeconfig"
  hostPath:
    path: "/etc/kubernetes/worker-kubeconfig.yaml"
- name: "etc-kube-ssl"
  hostPath:
    path: "/etc/kubernetes/ssl"
- hostPath:
    path: /var/run/dbus
  name: dbus
---

**Set up kubeconfig

  In order to facilitate secure communication between Node components and Master components, the kubeconfig file can be used to define authentication settings. In this case, the kubelet and proxy pod will reading this configuration to communicate with the API on the Controller.

  Create /etc/kubernetes/worker-kubeconfig.yaml:

---
  /etc/kubernetes/worker-kubeconfig.yaml

  apiVersion: v1
  kind: Config
  clusters:
  - name: local
    cluster:
      certificate-authority: /etc/kubernetes/ssl/ca.pem
  users:
  - name: kubelet
    user:
      client-certificate: /etc/kubernetes/ssl/worker.pem
      client-key: /etc/kubernetes/ssl/worker-key.pem
  contexts:
  - context:
      cluster: local
      user: kubelet
    name: kubelet-context
  current-context: kubelet-context
---

**Start Services

  We are now in a position to start the Worker services.

  Tell systemd to rescan the units on disk:

---
  sudo systemctl daemon-reload
---

***Start kubelet and flannel services

  Start the flannel and kubelet services (which will, in turn, start the proxy pod).

---
  sudo systemctl start flanneld

  sudo systemctl start kubelet
---

  Ensure that the services start on each boot:

---
  sudo systemctl enable flanneld

  sudo systemctl enable kubelet
---

**Basic Worker Health Checks

  To check the health of the kubelet systemd unit that we created

---
  systemctl status kubelet.service
---



*kubectl

  kubectl is the command line tool used for accessing/manipulating a running kubernetes cluster. It works by talking principally to the Api server on port 443 (via REST calls, as far as I can tell).

  Its basic usage is described in the {{{https://kubernetes.io/docs/reference/kubectl/overview/}kubectl overview}} pages. All deployments, etc are generally performed using kubectl.

  <<Note: You must use a kubectl version that is within one minor version difference of your cluster. For example, a v1.2 client should work with v1.1, v1.2, and v1.3 master. Using the latest version of kubectl helps avoid unforeseen issues>>

  Executables are available for both linux and osx, and for most scenarios executing from the client osx machine would seem like the natural choice.

  For osx, kubectl can be installed either via brew:

---
    brew install kubectl
---
  or via curl. First establish the latest release:

---
  curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt

  v1.17.0
---
  and then install

---
  curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.17.0/bin/linux/amd64/kubectl
---
  or, for OSX:

---
  curl -O https://storage.googleapis.com/kubernetes-release/release/v1.6.1/bin/darwin/amd64/kubectl
---

  Instinctively, the brew route seemed to be the way to go. However, this did cause difficulties ( see below). Brew will tend to install the latest version of the product, which is not always what you want.

  For instance, version 1.17(the current latest) does not work with server 1.5.4 which seems to be installed with this version of coreOs (beta 2303.2.0 )


kubectl configuration

  <<<kubectl>>> requires a configuration file to point it to the cluster in question, and supply authentication credentials etc.

  By defualt, this gets held within <<<~/.kube/config>>>. However, you can point it towards any particular file eiter by:

    * using the <<<---kubeconfig>>> commandline parameter;

    * setting the environment variable <<<KUBECONFIG>>>

    []
  e.g.

---
  export KUBECONFIG="$(pwd)/kubeconfig"
---

  (Note, that the various kubernetes server processes also use a similarily formatted file in order to find the api-server.)

  In simple terms, the config stores 4 things:

    * sets of server addresses

    * set of credentials that can be used to access seerers

    * set of contexts, which is basically an associateion betweeen a server and a set of credentials

    * the name of the current context.  i.e the server and credentials to be used for current operations.

    []

  A typical configuration file looks like this:

---
  apiVersion: v1
  clusters:
  - cluster:
      certificate-authority: ssl/ca.pem
      server: https://172.17.4.101:443
    name: vagrant-multi-cluster
  contexts:
  - context:
      cluster: vagrant-multi-cluster
      namespace: default
      user: vagrant-multi-admin
    name: vagrant-multi
  current-context: vagrant-multi
  kind: Config
  preferences: {}
  users:
  - name: vagrant-multi-admin
    user:
      client-certificate: ssl/admin.pem
      client-key: ssl/admin-key.pem
---
  The contents of the configuration file can itself be manipulated by kubectl

  For example:

---
  kubectl config set-cluster vagrant-multi-cluster \
          --server=https://172.17.4.101:443 \
          --certificate-authority=${PWD}/ssl/ca.pem


  kubectl config set-credentials vagrant-multi-admin \
       --certificate-authority=${PWD}/ssl/ca.pem \
       --client-key=${PWD}/ssl/admin-key.pem \
       --client-certificate=${PWD}/ssl/admin.pem

    User "vagrant-multi-admin" set.


  kubectl config set-context vagrant-multi  \
       --cluster=vagrant-multi-cluster \
       --user=vagrant-multi-admin

    Context "vagrant-multi" created.


  kubectl config use-context vagrant-multi

    Switched to context "vagrant-multi".
---

  The following command can be used to view the contents of the config file:

---
  kubectl config view

  apiVersion: v1
  clusters:
  - cluster:
      server: https://api.us-east-2.starter.openshift-online.com:6443
    name: api-us-east-2-starter-openshift-online-com:6443
  - cluster:
      server: 1.2.3.4
    name: tester
  contexts:
  - context:
      cluster: api-us-east-2-starter-openshift-online-com:6443
      user: kcrocombe/api-us-east-2-starter-openshift-online-com:6443
    name: /api-us-east-2-starter-openshift-online-com:6443/kcrocombe
  - context:
      cluster: api-us-east-2-starter-openshift-online-com:6443
      namespace: javatechnotes
      user: kcrocombe/api-us-east-2-starter-openshift-online-com:6443
    name: javatechnotes/api-us-east-2-starter-openshift-online-com:6443/kcrocombe
  - context:
      cluster: vagrant-multi-cluster
      user: vagrant-multi-admin
    name: vagrant-multi
  current-context: vagrant-multi
  kind: Config
  preferences: {}
  users:
  - name: kcrocombe/api-us-east-2-starter-openshift-online-com:6443
    user:
      token: BUZ-UUSzaCQGqIDhirv4ms1O2BaGXg3WqjlQcXXDxG4
  - name: vagrant-multi-admin
    user:
      client-certificate: /Users/kevin/dvl/gitReps/coreos-kubernetes/multi-node/vagrant/ssl/admin.pem
      client-key: /Users/kevin/dvl/gitReps/coreos-kubernetes/multi-node/vagrant/ssl/admin-key.pem
---

Useful kubectl commands

  Note: if kubectl responds:

---
Error from server (NotAcceptable): the server was unable to respond with a content type that the client supports (get nodes)
---
  this often means there is mismatch between the version the kubectl client and the version of the api-sserver. Check with <<<kubectl versions>>>.


---
  kubectl get nodes

  NAME           STATUS                     ROLES     AGE       VERSION
  172.17.4.101   Ready,SchedulingDisabled   <none>    1d        v1.9.11+coreos.0
  172.17.4.201   Ready                      <none>    1d        v1.9.11+coreos.0


  kubectl cluster-info

  Kubernetes master is running at https://172.17.4.101:443
  Heapster is running at https://172.17.4.101:443/api/v1/namespaces/kube-system/services/heapster/proxy
  KubeDNS is running at https://172.17.4.101:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
  Jkubernetes-dashboard is running at https://172.17.4.101:443/api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy

  kubectl cluster-info dump  > someFile.txt

  This dumps loads of information, too much to take in interactively, but eseful to send to a file.

  Includes configuration info, in json format.

  Logs from all the processes.



  kubectl attach POD -c CONTAINER [-i] [-t] [flags]

  kubectl version

  kubectl logs

  kubectl port-forward POD [LOCAL_PORT:]REMOTE_PORT [...[LOCAL_PORT_N:]REMOTE_PORT_N] [flags]

  kubectl run NAME --image=image [--env="key=value"] [--port=port] [--replicas=replicas] [--dry-run=bool] [--overrides=inline-json] [flags]

  kubectl apply -f FILENAME [flags]

  kubectl create/delete -f FILENAME [flags]

  kubectl edit

---






======

Getting kubernetes to run on the latest stable stream of Coreos.

  The basic problem here is that the coreos-cloudinit method of provisioning the box is being deprecated in favour of a Ignition based setup, and hasn't been maintained as the product moves forward.

  For the etcd servers:

    coreos-cloudinit can only set up etcd2 servers (and we have now moved on to etcd v3 running in a container)

  For the controllers and worker servers:

    vagrant doesn't inject the networks.yml and hostname.yml files that the contoller.sh and worker.sh scripts are expecting to be picked up an processed by coreos-cloudinit. As a consequence, the environment does not get set up correctly and the various build sctipts do not run.


  Fortunately it ought to be possible to adapt the provisioning scripts so that they can be used by Ignition rather than coreos-cloudinit.


*Possible ways forward

  Vagrant has a plugin that we can use to inject Ignition script into the host we are provisioning. CoreOS also provides a 'ct' ('configuration transpiler') tool to aid the generation of Ignition scripts from the intuitively simpler YAML.

  [[1]] Inject and Run the same, provided build scripts.

  [[1]] The build scripts largely use 'here' documents to install various configuration files in various places (described above). There is no reason why these can not be teased out from the 'here' documents and injected directly with Ignition.

  We will try the first route first, ad then maybe comback to the second.

*Changes to the Vagrant File

  The existing vagrant file used its own provisioning mechanism to push files to the box AFTER it had been booted for the first time. These were then picked up by coreos-cloudinit infrastructure and processed accordingly.

  Some of these injected files were pre-processed by the Vagrant files to inject ip addresses and other such variables and configuration.

  If we are to use ignition, such files will be injected by a different route.

  Ignition works on vagrant by way of a plugin (vagrant.ignition). Ignition provisions servers by processing ignition configuration files (which are json based). The vagrant plugin works by creating a virtual disk onto which is copied all the required ignition files, and then presents that virtual disk into the server being built.

  The ignition config files themselves are quite finicky, so noramlly they are generated by pre-processing a more freindly YAML version of the scripts. There is a ct utility which will transform the yaml into ign files.

  The Vagrant files thus needed modification to invoke ct and generate the ignition files.

  Additionally the Vagrant file was modifies to included the vagrant.plug itself and the additonal processing to make use of it.


*The Etcd servers

  An Ignition configuration for this already existed ast part of the coreos/vagrant git repository at:

    https://github.com/coreos/coreos-vagrant.git

  The configuration for this (cl.conf). This was adapted for use in this situation by:

    [[1]] Removing the unneccesary docker service

    [[1]] Disabling automatic updates when an os upgrade occurs

    [[1]] Adding in the specific flannel configuration that we were already using on the controller build

---
  /usr/bin/etcdctl set /flannel/network/config '{ "Network": "10.1.0.0/16" }'
---

  This was distilled into a single config file: etcd.ct injected into ignition as etcd.ign

  Previous versions of Vagrantfile used to pre-process the etcd configuration files in order to introduce hostname, cluster ip addresses etc. Becuase ignition has awareness of a certain numbe of environment variables, it has been possible to right a .ct file that does not need pre-processing.

  Instead, all vagrant does is run the ct utility in order to generate the .ign file. The Ignition plugin then post processes the .ign file to introduce the core user, certificates etc. and copies it to a virtual disk for presentation to the booting server.

  The general form of the ct invocation wold be:

---
  ct -in-file=etcd.ct -out-file=etcd.ign -platform vagrant-virtualbox -pretty -strict
---


*The Controller Servers

  Converting this server to Ignition was a bit more involved. The basic idea was:

    * inject a modified version of the controller-install.sh script

    * create a one-off sercice kubernetes-install to run the install script above on first boot.

    * directly inject the various ssl certificates

    * inject an etc/options.env file to allow a degree of customisation.

    []

  The IgnitionFile would be responsible for creating the options file, whose contents would include:

    * the ETC_ENPOINTS that the controller is to use.

    []

  As with the etcd server, the vagrant file would use the 'ct' utility to geenrate teh necessary .ign file and inject it in a similar fashion.


Have noticed the th controller serve can take a while to come up:

Somtimes it doesn;t come up at all - possibly it gives up if it can' register everything within a certain time?? I don;t know. Possibly starting ht server in osolation from hte etc and worker servers will help ghere??




systemctl status kubernetes-install
systemctl status kubelet

rkt list
UUID		APP		IMAGE NAME					STATE	CREATED		STARTED		NETWORKS
435b2d04	flannel		quay.io/coreos/flannel:v0.11.0			running	1 hour ago	1 hour ago
6fef3605	hyperkube	quay.io/coreos/hyperkube:v1.9.11_coreos.0	running	1 hour ago	1 hour ago



*The Worker Server

  Converting the worker server involves the same process as for the Controller server. Namely:

    * inject a modified version of the worker-install.sh script

    * create a one-off sercice kubernetes-install to run the install script above on first boot.

    * directly inject the various ssl certificates

    * inject an etc/options.env file to allow a degree of customisation.

    []

  In this case, however, we need to inject 2 options:

    * the ETC_ENDPOINTS that the worker is to use.

    * the Controllers api-server endpoint

    []



journalctl -u ignition-*

systemctl status kubernetes-install
journalctl -u kubernetes-install

systemctl status kubelet
journalctl -u kubelet





*Handling deprecated stuff,.

  Jan 09 19:00:15 c1 kubelet-wrapper[1206]: Flag --api-servers has been deprecated, Use --kubeconfig instead. Will be A
  removed in a future version.
  Jan 09 19:00:16 c1 kubelet-wrapper[1206]: Flag --rkt-stage1-imagekhas been deprecated, Will be removed in a future version. The default stage1 image will be specified by the rkt configurations, see https://github.com/coreos/rkt/blob/master/Documentation/configuration.md for more details


  The --rkt-stage1-image flag is a flag on the kubelet part of hyperkube. Its job is to specify the stage1 loader to be used by invoked by rkt, prior to running the container itself. In this case it is requesting the "standard" stage1-coreos loader which will layer in components allowing it to be managed by systemd.

  There is no replacement flag. Instead the loader is specified as one of the option to rkt itself ( namely --stage1-from-dir).

  However, it appears taht there is no need to explicitly introduce this flag. rkt is currently launched through the coreOs supplied wrapper /usr/lib64/coreos/kubelet-wrapper. This EXPLICITLY exists to introduce the --stage1-from-dir parameter and set it to stage1-fly.aci. (Note this is different from the depreciated --rkt-stage1-image setting).

  Previously, then it, would appear that rkt was bing invoked with 2 flags doing contradictory things. It is not clear which has precedent but my guess would be the one introdued by kubelet-wrapper.

  It is also indicated in the documentation that the 'fly' loader IS preferred for kubernetes. Thus the only chnge we need to make is to remove teh redundnat flag.



kubelet-wrapper
  #!/bin/bash
# Wrapper for launching kubelet via rkt-fly.


--rkt-stage1-image=coreos.com/rkt/stage1-coreos


---
--stage1-from-dir=stage1-coreos.aci

--stage1-from-dir=stage1-fly.aci

--kubeconfig=/path/to/kubeconfig --require-kubeconfig

e.g.

 --api-servers=http://127.0.0.1:8080 \
--kubeconfig=/etc/kubernetes/master-kubeconfig.yaml \
--require-kubeconfig \


**/etc/kubernetes/master-kubeconfig.yaml**

apiVersion: v1
kind: Config
clusters:
- name: local
  cluster:
    server: http://127.0.0.1:8080
users:
- name: kubelet
contexts:
- context:
    cluster: local
    user: kubelet
  name: kubelet-context
current-context: kubelet-context
---

https://github.com/coreos/coreos-kubernetes/compare/master...euank:kubeconfig?expand=1



https://github.com/kelseyhightower/kubernetes-the-hard-way





RKT

rkt ( rocket) is the Container runtime developed and packaged by coreOS. There are some difference between rkt and docker:

  * rkt is daemonless. The problem of containers dyinf is the dockerd is restarted does not exist.

  * rkt integrates well with systemd so that containers resources limits can be set easily for containers.


  There aer 3 stages of rkt exaecuton

  * Stage 0 : This does image discaover and retrieval. It also sets up teh filesystem  for stages 1 nad 2.

  * Stage 1 : This sets up the executionenvironemnt for the container executon using the filesystem set up be stage 0. Rkt sets up cgroups, networking etc. There are more than implementiation of the the stage1 loader.


      - the regular one : which uses systemd

      - the lightweight one : fly

  * Stage2 This is the actual execution of the Cntainer pod and the appliaciton itself.


  rkt's execution of pods is divided roughly into three separate stages:

    Stage 0: discovering, fetching, verifying, storing, and compositing of both application (stage2) and stage1 images for execution.
    Stage 1: execution of the stage1 image from within the composite image prepared by stage0.
    Stage 2: execution of individual application images within the containment afforded by stage1.

This separation of concerns is reflected in the file-system and layout of the composite image prepared by stage0:

    Stage 0: rkt executable, and the pod manifest created at /var/lib/rkt/pods/prepare/$uuid/pod.
    Stage 1: stage1.aci, made available at /var/lib/rkt/pods/run/$uuid/stage1 by rkt run.
    Stage 2: $app.aci, made available at /var/lib/rkt/pods/run/$uuid/stage1/rootfs/opt/stage2/$appname by rkt run, where $appname is the name of the app in the pod manifest.

  The stage1 implementation is what creates the execution environment for the contained applications. This occurs via entrypoints from stage0 on behalf of rkt run and rkt enter. These entrypoints are executable programs located via annotations from within the stage1 ACI manifest, and executed from within the stage1 of a given pod at /var/lib/rkt/pods/$state/$uuid/stage1/rootfs.

  Stage2 is the deployed application image. Stage1 is the vehicle for getting there from stage0. For any given pod instance, stage1 may be replaced by a completely different implementation. This allows users to employ different containment strategies on the same host running the same interchangeable ACIs.


  The motivation of the fly feature is to add the ability to run applications with full privileges on the host but still benefit from the image management and discovery from rkt. The Kubernetes kubelet is one candidate for rkt fly.

  n comparison to the default stage1, there is no process manager involved in the stage1. This a visual illustration for the differences in the process tree between the default and the fly stage1:

  stage1-coreos.aci:

  host OS
    └─ rkt
      └─ systemd-nspawn
        └─ systemd
          └─ chroot
            └─ user-app1

  stage1-fly.aci:

  host OS
    └─ rkt
      └─ chroot
        └─ user-app1

  The rkt application sets up bind mounts for /dev, /proc, /sys, and the user-provided volumes. In addition to the bind mounts, an additional tmpfs mount is done at /tmp. After the mounts are set up, rkt chroots to the application's RootFS and finally executes the application.


  By design, the fly stage1 does not provide the same isolaton and security features as the default stage1.

  Specifically, the following constraints are not available when using the fly stage1:

    network namespace isolation
    CPU isolators
    Memory isolators
    CAPABILITY bounding
    SELinux



*RKT - To force garbage collection

    rkt gc --grace-period 0



  stage 0

  stage 1

  stage 2


Updating the Version of kubernetes

  Initially, just amended the version of the Hyperkube image in the kubelet service file and rebooted the server.

  This seemed to work.

    kubelet get pods

    NAME           STATUS                     AGE       VERSION
172.17.4.101   Ready,SchedulingDisabled   18m       v1.9.11+coreos.0
172.17.4.201   Ready                      11m       v1.9.11+coreos.0

---
which looked ok...

However

./kubectl version
Client Version: version.Info{Major:"1", Minor:"9", GitVersion:"v1.9.11",....
Server Version: version.Info{Major:"1", Minor:"9", GitVersion:"v1.5.4_coreos.0",...

Could not understand why this did not reflect the value on the upgraded server. It turned out to be more than cosmetic significane though...

If the client and server versions differ by too much they don't work together.

Consequently attempt to apply the test cases below failed with mesages sucj like
---



Installation of the the GuestBook test applicaion.

  Finding out how to access the exposed service:

---
  kubectl get service frontend

  NAME       TYPE       CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
  frontend   NodePort   10.3.0.95    <none>        80:31297/TCP   4s
---

  The application will then be available on <IP of WORKERNODE>:31279

      http://172.17.4.201:31297/

  But not on port 80 however....

  Need to understand this more.



Running the Conformance Tests

  These are resource intensive tests. The tests clone huge amounts of infrastructure, including the full kubernetes release, plus umteen other things needed to support the testing.

  These get written to the control server. In order to install at all the server needs to be configured withe:

    4 Gb of memory

    4cpu.

  For this eary part, when the infrastructure is being built, the worker and etcd servers are not needed however.

  What the script does:

  [[1]] performs a git clone on the github.com/coreos/kubernetes/v1.9.11+coreos.0 release --> ./go/src/k8sio/kubernetes

  [[1]] creates a kubconfic file

  [[1]] Downloads a docker image docker://golang.1.9.1 and creates a RKT pod based on it. From with it it runs a bash script, that:

    [[1]] uses apt-get update to refresh the list of latest available versions

    [[1]] installs rsync. ( Rsync syncs files between local/remote locations, but by only tranferring the bits that have changed).

    [[1]] Runs go to fetch back go-bindata

        go get -u github.com/jteeuwen/go-bindata/go-bindata

    [[1]] Creates a new branch on git

      git checkout v1.9.11+coreos.0

    [[1]] 'makes' kubectl

    [[1]] 'makes' vendor/github.com/onsi/ginkgo/ginkgo

    [[1]] 'makes' test/e2e/e2e.test

    [[1]] Runs the actual test suite

        go run hack/e2e.go -v --test -check_version_skew=false --test_args=\"ginkgo.focus='\[Conformance\]'\""

        This is still running within the go rkt container.

        ( Note that the -v flag has been depreciated)

  [[]]

  Currently this doesn't work. The e2e.go script always looks to rebuild the kubetest utility. One of the repositories this uses is aws/aws-k8tester. One of the files that kubetest requires (ekstester) has been dropped from the latest versions of aws-k8tester.  Consequenty, it no longer build. kubetest insists on checking out the latest versions of everything so there is no opportunity to substitute a earlier version.

  Even the tests that were working a few weeks ago, no longer work.

  It does look as though they are in process of introducing a wholly reworked kubetest, so it may be that we have to wait for them to sort it out. Other people are compklaining of the same problem.


The AddOns
that happen late in the build!

What are these services all about
  kubernetes master is running at https://172.17.4.101:443
  Heapster is running at https://172.17.4.101:443/api/v1/namespaces/kube-system/services/heapster/proxy
  KubeDNS is running at https://172.17.4.101:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
  Jkubernetes-dashboard is running at https://172.17.4.101:443/api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy
